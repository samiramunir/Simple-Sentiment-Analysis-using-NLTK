{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-17 10:19:57--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘aclImdb_v1.tar.gz’\n",
      "\n",
      "aclImdb_v1.tar.gz   100%[===================>]  80,23M   810KB/s    in 3m 23s  \n",
      "\n",
      "2019-05-17 10:23:21 (405 KB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "local_zip = \"aclImdb_v1.tar.gz\"\n",
    "tf = tarfile.open(local_zip)\n",
    "tf.extractall()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating paths to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_path = 'aclImdb'\n",
    "train_positive_path = os.path.join(data_path, 'train/pos/')\n",
    "train_negative_path = os.path.join(data_path,'train/neg/')\n",
    "print(train_positive_path, '\\n', train_negative_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "files_pos = os.listdir(train_positive_path)\n",
    "files_pos = [open(train_positive_path+f, 'r').read() for f in files_pos]\n",
    "files_neg = os.listdir(train_negative_path)\n",
    "files_neg = [open(train_negative_path+f, 'r').read() for f in files_neg]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(files_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "Download the packages:\n",
    "* stopwords\n",
    "* punkt\n",
    "* averaged_perceptron_tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "all_words = []\n",
    "documents = []\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "#  j is adject, r is adverb, and v is verb\n",
    "#allowed_word_types = [\"J\",\"R\",\"V\"]\n",
    "allowed_word_types = [\"J\"]\n",
    "\n",
    "for p in  files_pos:\n",
    "    \n",
    "    # create a list of tuples where the first element of each tuple is a review\n",
    "    # the second element is the label\n",
    "    documents.append( (p, \"pos\") )\n",
    "    \n",
    "    # remove punctuations\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)\n",
    "    \n",
    "    # tokenize \n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "    # remove stopwords \n",
    "    stopped = [w for w in tokenized if not w in stop_words]\n",
    "    \n",
    "    # parts of speech tagging for each word \n",
    "    pos = nltk.pos_tag(stopped)\n",
    "    \n",
    "    # make a list of  all adjectives identified by the allowed word types list above\n",
    "    for w in pos:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n",
    "\n",
    "    \n",
    "for p in files_neg:\n",
    "    # create a list of tuples where the first element of each tuple is a review\n",
    "    # the second element is the label\n",
    "    documents.append( (p, \"neg\") )\n",
    "    \n",
    "    # remove punctuations\n",
    "    cleaned = re.sub(r'[^(a-zA-Z)\\s]','', p)\n",
    "    \n",
    "    # tokenize \n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    \n",
    "    # remove stopwords \n",
    "    stopped = [w for w in tokenized if not w in stop_words]\n",
    "    \n",
    "    # parts of speech tagging for each word \n",
    "    neg = nltk.pos_tag(stopped)\n",
    "    \n",
    "    # make a list of  all adjectives identified by the allowed word types list above\n",
    "    for w in neg:\n",
    "        if w[1][0] in allowed_word_types:\n",
    "            all_words.append(w[0].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pos_A = []\n",
    "for w in pos:\n",
    "    if w[1][0] in allowed_word_types:\n",
    "        pos_A.append(w[0].lower())\n",
    "pos_N = []\n",
    "for w in neg:\n",
    "    if w[1][0] in allowed_word_types:\n",
    "        pos_N.append(w[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(pos_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "text = ' '.join(pos_A)\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "plt.figure(figsize = (15, 9))\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation= \"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# pickling the list documents to save future recalculations \n",
    "\n",
    "save_documents = open(\"pickled_algos/documents.pickle\",\"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# creating a frequency distribution of each adjectives. \n",
    "BOW = nltk.FreqDist(all_words)\n",
    "BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# listing the 5000 most frequent words\n",
    "word_features = list(BOW.keys())[:5000]\n",
    "word_features[0], word_features[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "save_word_features = open(\"pickled_algos/word_features5k.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function to create a dictionary of features for each review in the list document.\n",
    "# The keys are the words in word_features \n",
    "# The values of each key are either true or false for wether that feature appears in the review or not\n",
    "def find_features(document):\n",
    "    words = word_tokenize(document)\n",
    "    features = {}\n",
    "    for w in word_features:\n",
    "        features[w] = (w in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Creating features for each review\n",
    "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n",
    "\n",
    "# Shuffling the documents \n",
    "random.shuffle(featuresets)\n",
    "print(len(featuresets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_set = featuresets[:20000]\n",
    "testing_set = featuresets[20000:]\n",
    "print( 'training_set :', len(training_set), '\\ntesting_set :', len(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "\n",
    "print(\"Classifier accuracy percent:\",(nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "\n",
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Printing the most important features \n",
    "\n",
    "mif = classifier.most_informative_features()\n",
    "\n",
    "mif = [a for a,b in mif]\n",
    "print(mif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "# The first elemnt of the tuple is the feature set and the second element is the label \n",
    "ground_truth = [r[1] for r in testing_set]\n",
    "\n",
    "preds = [classifier.classify(r[0]) for r in testing_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1_score(ground_truth, preds, labels = ['neg', 'pos'], average = 'micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "y_test = ground_truth\n",
    "y_pred = preds\n",
    "class_names = ['neg', 'pos']\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB,BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifiers for an ensemble model: \n",
    "Naive Bayes (NB)\n",
    "Multinomial NB\n",
    "Bernoulli NB\n",
    "Logistic Regression\n",
    "Stochastic Gradient Descent Classifier - SGD\n",
    "Support Vector Classification - SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "MNB_clf = SklearnClassifier(MultinomialNB())\n",
    "MNB_clf.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_clf, testing_set))*100)\n",
    "\n",
    "BNB_clf = SklearnClassifier(BernoulliNB())\n",
    "BNB_clf.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BNB_clf, testing_set))*100)\n",
    "\n",
    "LogReg_clf = SklearnClassifier(LogisticRegression())\n",
    "LogReg_clf.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogReg_clf, testing_set))*100)\n",
    "\n",
    "SGD_clf = SklearnClassifier(SGDClassifier())\n",
    "SGD_clf.train(training_set)\n",
    "print(\"SGDClassifier_classifier accuracy percent:\", (nltk.classify.accuracy(SGD_clf, testing_set))*100)\n",
    "\n",
    "SVC_clf = SklearnClassifier(SVC())\n",
    "SVC_clf.train(training_set)\n",
    "print(\"SVC_classifier accuracy percent:\", (nltk.classify.accuracy(SVC_clf, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing all models using pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_pickle(c, file_name): \n",
    "    save_classifier = open(file_name, 'wb')\n",
    "    pickle.dump(c, save_classifier)\n",
    "    save_classifier.close()\n",
    "\n",
    "classifiers_dict = {'ONB': [classifier, 'pickled_algos/ONB_clf.pickle'],\n",
    "                    'MNB': [MNB_clf, 'pickled_algos/MNB_clf.pickle'],\n",
    "                    'BNB': [BNB_clf, 'pickled_algos/BNB_clf.pickle'],\n",
    "                    'LogReg': [LogReg_clf, 'pickled_algos/LogReg_clf.pickle'],\n",
    "                    'SGD': [SGD_clf, 'pickled_algos/SGD_clf.pickle'], \n",
    "                    'SVC': [SVC_clf, 'pickled_algos/SVC_clf.pickle']}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    create_pickle(listy[0], listy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc_scores = {}\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    # getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "    # The first elemnt of the tuple is the feature set and the second element is the label \n",
    "    acc_scores[clf] = accuracy_score(ground_truth, predictions[clf])\n",
    "    print(f'Accuracy_score {clf}: {acc_scores[clf]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "ground_truth = [r[1] for r in testing_set]\n",
    "predictions = {}\n",
    "f1_scores = {}\n",
    "for clf, listy in classifiers_dict.items(): \n",
    "    # getting predictions for the testing set by looping over each reviews featureset tuple\n",
    "    # The first elemnt of the tuple is the feature set and the second element is the label \n",
    "    predictions[clf] = [listy[0].classify(r[0]) for r in testing_set]\n",
    "    f1_scores[clf] = f1_score(ground_truth, predictions[clf], labels = ['neg', 'pos'], average = 'micro')\n",
    "    print(f'f1_score {clf}: {f1_scores[clf]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.classify import ClassifierI\n",
    "\n",
    "# Defininig the ensemble model class \n",
    "\n",
    "class EnsembleClassifier(ClassifierI):\n",
    "    \n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "    \n",
    "    # returns the classification based on majority of votes\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "    # a simple measurement the degree of confidence in the classification \n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        choice_votes = votes.count(mode(votes))\n",
    "        conf = choice_votes / len(votes)\n",
    "        return conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all models using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load all classifiers from the pickled files \n",
    "\n",
    "# function to load models given filepath\n",
    "def load_model(file_path): \n",
    "    classifier_f = open(file_path, \"rb\")\n",
    "    classifier = pickle.load(classifier_f)\n",
    "    classifier_f.close()\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# Original Naive Bayes Classifier\n",
    "ONB_Clf = load_model('pickled_algos/ONB_clf.pickle')\n",
    "\n",
    "# Multinomial Naive Bayes Classifier \n",
    "MNB_Clf = load_model('pickled_algos/MNB_clf.pickle')\n",
    "\n",
    "\n",
    "# Bernoulli  Naive Bayes Classifier \n",
    "BNB_Clf = load_model('pickled_algos/BNB_clf.pickle')\n",
    "\n",
    "# Logistic Regression Classifier \n",
    "LogReg_Clf = load_model('pickled_algos/LogReg_clf.pickle')\n",
    "\n",
    "# Stochastic Gradient Descent Classifier\n",
    "SGD_Clf = load_model('pickled_algos/SGD_clf.pickle')\n",
    "\n",
    "\n",
    "\n",
    "# Initializing the ensemble classifier \n",
    "ensemble_clf = EnsembleClassifier(ONB_Clf, MNB_Clf, BNB_Clf, LogReg_Clf, SGD_Clf)\n",
    "\n",
    "# List of only feature dictionary from the featureset list of tuples \n",
    "feature_list = [f[0] for f in testing_set]\n",
    "\n",
    "# Looping over each to classify each review\n",
    "ensemble_preds = [ensemble_clf.classify(features) for features in feature_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "f1_score(ground_truth, ensemble_preds, average = 'micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Sentiment Analysis\n",
    "\n",
    "Using the sentiment function we can classify individual reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to do classification a given review and return the label a\n",
    "# and the amount of confidence in the classifications\n",
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    return ensemble_clf.classify(feats), ensemble_clf.confidence(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sentiment analysis of reviews of captain marvel found on rotten tomatoes\n",
    "text_a = '''The problem is with the corporate anticulture that controls these productions-and \n",
    "            the fandom-targeted demagogy that they're made to fulfill-which responsible casting \n",
    "                can't overcome alone.'''\n",
    "text_b = '''Does it work? The short answer is: yes. There's enough to keep both diehard \n",
    "                Marvel fans and newcomers engaged.'''\n",
    "text_c = '''It was lacking, a bit flat, and I'm honestly concerned about how she will enter\n",
    "            the Marvel Cinematic Universe...it's so concerned with being a feminist film that \n",
    "            it forgets how to be a superhero movie.'''\n",
    "text_d = '''The film may be about women breaking their shackles, but the lead actress feels kept \n",
    "            in check for much of the picture. Humor winds up being provided by Samuel Jackson's Nick \n",
    "            Fury, heart by Lashana Lynch's Maria Rambeau, and pathos by...well, it ain't Larson'''\n",
    "text_e = '''\"Everything was beautiful and nothing hurt\"'''\n",
    "\n",
    "sentiment(text_a), sentiment(text_b), sentiment(text_c), sentiment(text_d), sentiment(text_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# converting the training set  into a pandas data frame\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "import pandas as pd \n",
    "df = pd.DataFrame([training_set[0][0]])\n",
    "for f in tqdm(training_set[1:]): \n",
    "    df = df.append([f[0]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# converting the testing set  into a pandas data frame\n",
    "df_test = pd.DataFrame([testing_set[0][0]])\n",
    "for f in tqdm(testing_set[1:]): \n",
    "    df_test = df_test.append([f[0]], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, min_samples_split = 100, random_state=0)\n",
    "X = df\n",
    "y = [x[1] for x in training_set]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.fit(X, y )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_test = df_test\n",
    "y_test = [x[1] for x in testing_set]\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Model (can also use single decision tree)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "# Train\n",
    "model.fit(iris.data, iris.target)\n",
    "# Extract single tree\n",
    "estimator = clf.estimators_[5]\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "# Export as dot file\n",
    "export_graphviz(estimator, out_file='tree.dot', \n",
    "                feature_names = df.columns,\n",
    "                class_names = ['neg', 'pos'],\n",
    "                rounded = True, proportion = False, \n",
    "                precision = 2, filled = True)\n",
    "\n",
    "# Convert to png using system command (requires Graphviz)\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in jupyter notebook\n",
    "from IPython.display import Image\n",
    "Image(filename = 'tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "clf.decision_path(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
